---
title: "558_Project_3"
author: "ankit gupta & brian higginbotham"
date: "2023-11-08"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, error = FALSE)
```

# Introduction

# Data

# Summarizations

# Modeling

## LogLoss

LogLoss is a performance measurement that measures the distance between correct outcome (1 or 0) and the calculated probability. So the closer the model prediction is to the actual outcome, the lower the LogLoss score is. 

LogLoss can be a better way of measuring the performance as compared to an Accuracy rating. If a model predicted 0.51 probability as True and the actual result was True, this would be marked as a correct outcome in an accuracy rating. But in reality, the model barely got the prediction right and this may result in poor performance when the model is implemented on new data. LogLoss records this measured discrepancy and thus may provide a better description of the models performance. LogLoss is similar to the Residual Mean Squared Error (RMSE) produced in linear regression models.

## Logistic Regression

Logistic Regression is a Generalized Linear Model that models the probability of a binary outcome (success or failure, 1 or 0, etc). A Simple Linear Regression model would produce a continuous response on the real line that would not correspond to the predictions we are trying to make, so we use a logistic function that produces a logit or log-odds of success that is linear in its parameters. The logit or log-odds is then used to determine the outcome, generally split at probability 0.5 (greater than 0.5 = success, less than 0.5 = failure).

## Random Forest

Random Forest utilizes bootstrap aggregation to calculate the average response over many fitted trees. Whereas a Classification and Bagged Tree model will use all predictors in modeling, Random Forest will use a random subset of predictors for each bootstrap sample. By randomly selecting a subset of predictors, a good predictor or two will not dominate the tree fits and thus the variance from aggregation will not be reduced, resulting in a more robust model.

## Linear Discriminant Analysis

Linear Discriminant Analysis identifies the most discriminitive features of the dataset to enhance classification accuracy. It projects the data onto a lower dimensional space (dimensionality reduction) and finds a linear combination of variables that best separates the classes in the dataset. One of it's advantages is that it can handle multicollineartity within the data. However, a few disadvantages are that it assumes a normal distribution and that the data is linearly separable - both of which may not always be the case.



